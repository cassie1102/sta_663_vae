{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(663)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "#    - requires_grad = True: compute the derivative of the activation function for backpropagation\n",
    "\n",
    "def sigmoid(x, requires_grad=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    res = 1/(1+np.exp(-x))\n",
    "    if requires_grad:\n",
    "        return res*(1-res)\n",
    "    return res\n",
    "\n",
    "def relu(x, requires_grad=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if requires_grad:\n",
    "        return 1.0 * (x > 0)\n",
    "    else:\n",
    "        return np.maximum(0, x)    \n",
    "\n",
    "def tanh(x, requires_grad=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if requires_grad:\n",
    "        return 1.0 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss function\n",
    "eps = 10e-8 # for numerical stability\n",
    "def BCE_loss(xhat, x):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    loss = np.sum(-x * np.log(xhat + eps) - (1 - x) * np.log(1 - xhat + eps))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE objective\n",
    "def loss_function(xhat, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    ELBO \n",
    "    \"\"\"\n",
    "    BCE = BCE_loss(xhat, x)\n",
    "    KL = -0.5 * np.sum(1 + logvar - mu ** 2 - np.exp(logvar))\n",
    "    return BCE + KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use argparse() to input the params later\n",
    "n_epoch = 20\n",
    "batch_size = 16\n",
    "lr = 0.01\n",
    "dim_z = 20\n",
    "layer_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.input_size = input_size\n",
    "        self.dim_z = dim_z\n",
    "        self.layer_size = layer_size # size of hidden layer\n",
    "                 \n",
    "        ### initialize encoder weights\n",
    "        self.enc_W0 = np.random.randn(self.input_size, self.layer_size).astype(np.float64) # numerical stability?\n",
    "        self.enc_b0 = np.zeros(self.input_size, self.layer_size).astype(np.float64)\n",
    "        \n",
    "        self.mu_W = np.random.randn(self.layer_size, self.dim_z).astype(np.float64)\n",
    "        self.mu_b = np.zeros(self.layer_size, self.dim_z).astype(np.float64)\n",
    "        \n",
    "        self.logvar_W = np.random.randn(self.layer_size, self.dim_z).astype(np.float64)\n",
    "        self.logvar_b = np.zeros(self.layer_size, self.dim_z).astype(np.float64)\n",
    "        \n",
    "        ### initialize decoder weights\n",
    "        self.dec_W0 = np.random.randn(self.dim_z, self.input_size).astype(np.float64)\n",
    "        self.dec_b0 = np.zeros(self.dim_z, self.input_size).astype(np.float64)\n",
    "        self.dec_W1 = np.random.randn(self.dim_z, self.input_size).astype(np.float64)\n",
    "        self.dec_b1 = np.zeros(self.dim_z, self.input_size).astype(np.float64)\n",
    "        \n",
    "        ### initialize optimizer\n",
    "                 \n",
    "                 \n",
    "    def encode(self, x):\n",
    "          \n",
    "        self.input =  np.reshape(self.x, (self.batch_size, -1))\n",
    "        self.enc_h0 = np.dot(self.input, self.enc_W0) + self.enc_b0\n",
    "        self.enc_h0 = relu(self.enc_h0)\n",
    "        self.mu = np.dot(self.input, self.mu_W) + self.mu_b\n",
    "        self.logvar = np.dot(self.input, self.logvar_W) + self.logvar_b\n",
    "                \n",
    "        return self.mu, self.logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \n",
    "        # maybe no need to sample from gaussian?\n",
    "        std = np.exp(0.5 * logvar)\n",
    "        eps = np.random.randn(*std.shape)\n",
    "        return mu + eps * std\n",
    "                \n",
    "    def decode(self, z):\n",
    "        \n",
    "        self.z = np.reshape(self.z, (self.batch_size, self.dim_z))\n",
    "        self.dec_h0 = np.dot(self.z, self.dec_W0) + self.dec_b0\n",
    "        self.dec_h0 = sigmoid(self.dec_h0)\n",
    "        self.dec_h1 = np.dot(self.z, self.dec_W1) + self.dec_b1\n",
    "        self.dec_h1 = sigmoid(self.dec_h1)\n",
    "        # self.out = reshape output here according to BCE_loss \n",
    "              \n",
    "        return self.out\n",
    "                \n",
    "    def feedforward(self, x):\n",
    "        \n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "                \n",
    "        return recon_x, mu, logvar\n",
    "    \n",
    "    def backpropagation(self, x, out):\n",
    "                 \n",
    "        # Compute gradients\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "        # Update weights\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "    def train(self):\n",
    "        \n",
    "        ### set up\n",
    "                 \n",
    "            \n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx in range(self.batch_size):\n",
    "                \n",
    "                ### feedforward\n",
    "                \n",
    "                \n",
    "                \n",
    "                ### backpropagation\n",
    "                 \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model = VAE(# add extra params)\n",
    "    model.train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
